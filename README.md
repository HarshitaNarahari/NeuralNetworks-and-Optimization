# NeuralNetworks-and-Optimization
This repository contains my code which uses both SGD(Stochastic Gradient Descent) and Adam optimizers to optimize the Rosenbrock, Beale, and Ackley functions using PyTorch and implements and trains a simple Feed Forward Neural Network. 
